{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "936c6c87",
      "metadata": {
        "id": "936c6c87"
      },
      "source": [
        "![image.png](https://i.imgur.com/a3uAqnb.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "824b7295",
      "metadata": {
        "id": "824b7295"
      },
      "source": [
        "# **ğŸš€ Object Detection with Faster R-CNN**\n",
        "In this lab, we will:\n",
        "\n",
        "âœ… Build a **custom Dataset class** for **Pascal VOC dataset**  \n",
        "âœ… Use a **pretrained Faster R-CNN model** for object detection  \n",
        "âœ… Train and evaluate the model  \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8df1282f",
      "metadata": {
        "id": "8df1282f"
      },
      "source": [
        "## **1ï¸âƒ£ Dataset Class**\n",
        "We use the **Pascal VOC 2007 dataset**, which contains images with **bounding boxes and labels**.  \n",
        "PyTorch provides a built-in dataset loader: `torchvision.datasets.VOCDetection`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f60e0b8",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4f60e0b8",
        "outputId": "bb5bbb4c-52c9-45c7-c828-1a1c96c00de8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar to ./VOC_data/VOCtrainval_06-Nov-2007.tar\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 460M/460M [00:21<00:00, 21.6MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./VOC_data/VOCtrainval_06-Nov-2007.tar to ./VOC_data/\n",
            "Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar to ./VOC_data/VOCtest_06-Nov-2007.tar\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 451M/451M [00:23<00:00, 19.6MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./VOC_data/VOCtest_06-Nov-2007.tar to ./VOC_data/\n"
          ]
        }
      ],
      "source": [
        "### **ğŸ”¹ Load & Transform Dataset**\n",
        "import torchvision\n",
        "from torchvision.datasets import VOCDetection\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "# Define dataset path and transformations\n",
        "data_path = \"./VOC_data/\"\n",
        "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
        "\n",
        "# Load Pascal VOC dataset (train & test)\n",
        "train_dataset = VOCDetection(root=data_path, year='2007', image_set='train', download=True, transform=transform)\n",
        "test_dataset = VOCDetection(root=data_path, year='2007', image_set='test', download=True, transform=transform)\n",
        "\n",
        "# Custom collate function to handle variable number of bounding boxes\n",
        "def custom_collate_fn(data): # handel the dynmic of data set and ÙŠÙØ±Ù‚ Ø¨ÙŠÙ†Ù‡Ù… seprated in\n",
        "    return tuple(zip(*data))\n",
        "\n",
        "# Create DataLoaders                 (Tip: Use lower batch size if encountered Out-of-Memory (OOM) error in Training)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=2, collate_fn=custom_collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=2, collate_fn=custom_collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63daca94",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "63daca94"
      },
      "outputs": [],
      "source": [
        "# Mappings of label names (found in dataset annotation) to integer IDs (or classes) which we will feed to the model\n",
        "voc_classes = {\n",
        "    \"aeroplane\": 0,\n",
        "    \"bicycle\": 1,\n",
        "    \"bird\": 2,\n",
        "    \"boat\": 3,\n",
        "    \"bottle\": 4,\n",
        "    \"bus\": 5,\n",
        "    \"car\": 6,\n",
        "    \"cat\": 7,\n",
        "    \"chair\": 8,\n",
        "    \"cow\": 9,\n",
        "    \"diningtable\": 10,\n",
        "    \"dog\": 11,\n",
        "    \"horse\": 12,\n",
        "    \"motorbike\": 13,\n",
        "    \"person\": 14,\n",
        "    \"pottedplant\": 15,\n",
        "    \"sheep\": 16,\n",
        "    \"sofa\": 17,\n",
        "    \"train\": 18,\n",
        "    \"tvmonitor\": 19,\n",
        "}\n",
        "\n",
        "#  Reverse of label to class id mapping. needed because the model predictions will be ids and we need to change it to label to visualize it.\n",
        "reverse_voc_classes = {v: k for k, v in voc_classes.items()}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7efd317",
      "metadata": {
        "id": "a7efd317"
      },
      "source": [
        "### **ğŸ”¹ Why Do We Need a Custom `collate_fn`?**\n",
        "Unlike classification datasets, where each image has a **fixed shape and label**, object detection images have **variable numbers of bounding boxes**.  \n",
        "\n",
        "- The default `collate_fn` (which applies `torch.stack`) **doesn't work**, since bounding box tensors have different shapes.  \n",
        "- Instead, we **return a tuple** that **keeps individual image-label pairs separate**.\n",
        "\n",
        "##### Before using custom collate_fn:\n",
        "```python\n",
        "data = [\n",
        "    (image1, dict1),  \n",
        "    (image2, dict2),\n",
        "    ...  \n",
        "]\n",
        "```\n",
        "##### After:\n",
        "```python\n",
        "images_tuple = (image1, image2, ...)  \n",
        "targets_tuple = (dict1, dict2, ...)   \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47cd40bf",
      "metadata": {
        "id": "47cd40bf"
      },
      "source": [
        "## **2ï¸âƒ£ Model Class**\n",
        "We use a **pretrained Faster R-CNN with a MobileNetV3-Large backbone**.\n",
        "\n",
        "### **ğŸ”¹ Modify the Model**\n",
        "- The default model is trained on **COCO dataset** with **91 classes**.\n",
        "- We modify the classifier to detect **20 Pascal VOC classes**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "524ed7ea",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "524ed7ea",
        "outputId": "af93b002-04e3-45b8-c999-30c27d4b21c7",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_mobilenet_v3_large_fpn-fb6a3cc7.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_mobilenet_v3_large_fpn-fb6a3cc7.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74.2M/74.2M [00:00<00:00, 99.8MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "FasterRCNN(\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              "  (backbone): BackboneWithFPN(\n",
              "    (body): IntermediateLayerGetter(\n",
              "      (0): Conv2dNormActivation(\n",
              "        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
              "        (2): Hardswish()\n",
              "      )\n",
              "      (1): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
              "            (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(64, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
              "            (1): FrozenBatchNorm2d(64, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): Conv2dNormActivation(\n",
              "            (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(24, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
              "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): Conv2dNormActivation(\n",
              "            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(24, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
              "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
              "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (6): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
              "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (7): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(240, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
              "            (1): FrozenBatchNorm2d(240, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): Conv2dNormActivation(\n",
              "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (8): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(200, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
              "            (1): FrozenBatchNorm2d(200, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): Conv2dNormActivation(\n",
              "            (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
              "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): Conv2dNormActivation(\n",
              "            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (10): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
              "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): Conv2dNormActivation(\n",
              "            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (11): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(480, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "            (1): FrozenBatchNorm2d(480, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(112, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (12): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
              "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(112, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (13): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
              "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (14): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
              "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (15): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
              "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (16): Conv2dNormActivation(\n",
              "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
              "        (2): Hardswish()\n",
              "      )\n",
              "    )\n",
              "    (fpn): FeaturePyramidNetwork(\n",
              "      (inner_blocks): ModuleList(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(160, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (layer_blocks): ModuleList(\n",
              "        (0-1): 2 x Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (extra_blocks): LastLevelMaxPool()\n",
              "    )\n",
              "  )\n",
              "  (rpn): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (cls_logits): Conv2d(256, 15, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(256, 60, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (roi_heads): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
              "    (box_head): TwoMLPHead(\n",
              "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=20, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=80, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torchvision\n",
        "\n",
        "# Load pretrained Faster R-CNN model\n",
        "model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True) # Ø¨Ø§ÙƒØ¬ Ù…ÙˆØ¯Ù„ Ø¨Ø¹Ø¯ÙŠÙ† Ù†Ø¨ÙŠ Ø¯ÙŠØ¯ÙƒØª Ùˆ Ø§ÙŠØ´ Ù‡Ùˆ\n",
        "\n",
        "# Change number of output classes to match Pascal VOC dataset Ù…ÙˆØ¯ÙÙŠ Ù„Ø§Ø³Øª Ù„Ø§ÙŠØ± Ø¹Ù„Ù‰ ÙƒÙ„Ø§Ø³Ø³\n",
        "num_classes = 20  # Pascal VOC has 20 object classes Ø¹Ù†Ø¯Ù†Ø§ ÙÙ‚Ø· Ø§ÙˆØª Ùˆ Ù†Ø¨ÙŠ Ø§ÙƒØ³Ø³ Ù„Ù„ Ø§Ù†Ø¨Øª Ø¹Ù„Ø´Ø§Ù† Ù†Ù‚Ø¯Ø± Ù†ØºÙŠØ± Ø§Ø®Ø± Ù„Ø§ÙŠØ±\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features  # Input features for predictor\n",
        "\n",
        "# Replace final layer with new predictor\n",
        "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "\n",
        "# Freeze the backbone and just finetune the head (You can finetune the whole model, but it'd take time and resources)  Ø¨ÙˆÙ† Ù…ÙˆØ¯Ù„ Ùˆ Ø§Ù„Ø±Ø§Ø³ ØªØ§Ø³Ùƒ Ø³Ø¨ÙŠØ³ÙÙƒ Ù„Ø§ÙŠØ± ÙÙ‚Ø· ÙŠØ¹Ø·ÙŠ ÙƒÙ„Ø§Ø³ÙÙŠØ±\n",
        "# Ù Ù†Ø«Ø¨Øª Ù…ÙˆØ¯Ù„ Ø§Ù„ÙŠ ÙÙŠÙ‡ ÙƒÙ„ ectract ÙˆÙƒÙŠØ°Ø§ and no need to update weight\n",
        "model.requires_grad_(False) # in backbones\n",
        "model.roi_heads.box_predictor = model.roi_heads.box_predictor.requires_grad_(True)  # in head we change the output so we need to calculate grediant\n",
        "\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "model ## Ù…Ù† Ù‡Ù†Ø§ Ù†Ø´ÙˆÙ Ù„Ø§ÙŠØ±"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "def5003f",
      "metadata": {
        "id": "def5003f"
      },
      "source": [
        "## **3ï¸âƒ£ Training and Validation Loops**\n",
        "### **ğŸ”¹ Training Loop**\n",
        "- The model takes **images & targets** is the bounding boxes (target) and computes **losses internally**. No need to define the loss.\n",
        "- We only need to **backpropagate and update the optimizer**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ed05f2b",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6ed05f2b"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for images, targets in tqdm(dataloader):\n",
        "        images = list(img.to(device) for img in images)  # its list to matches what model expecting this way ,, list of image in one batch\n",
        "\n",
        "        # Convert targets its bounding box and label inside it\n",
        "        for target in targets: # Ù‡Ù†Ø§ Ø¹Ù„ ÙƒÙ„ ØµÙˆØ±Ù‡ Ù†Ù…Ø´ÙŠ\n",
        "            boxes = []\n",
        "            labels = []\n",
        "            for obj in target['annotation']['object']: # annotation is the label # objech is the bounding box  # Ù‡Ù†Ø§ Ù†Ù…Ø´ÙŠ Ø¹Ù„ ÙƒÙ„ Ø¨ÙˆÙ†Ø¯Ù‚ Ø¨ÙˆÙƒØ³\n",
        "                label = obj['name']  # Ù‡Ù†Ø§ Ù„Ø§Ø¨Ù„ Ùˆ Ø¨ÙˆÙƒØ³ ÙÙŠ ØµÙˆØ±Ù‡ ÙˆØ­Ø¯Ù‡ ÙÙŠ Ù†ÙØ³ Ø§Ù„ØµÙˆØ±Ù‡ Ø¨Ø¹Ø¯ Ù…Ø§ Ù†Ø®Ù„Øµ Ø§ÙŠØªØ±Ø§ØªÙŠ Ø«Ø§Ù†ÙŠ\n",
        "                box = obj['bndbox']\n",
        "                xmin, ymin, xmax, ymax = [int(box[k]) for k in ['xmin', 'ymin', 'xmax', 'ymax']] # define box , Ù†Ø¯Ø®Ù„Ù‡ Ùƒ Ùˆ Ùƒ ÙÙŠ Ø¨ÙˆÙƒØ³ Ùˆ ÙŠØµÙŠØ± Ø§Ù†ØªØ¬Ø± Ùˆ ÙŠØ¯Ø®Ù„ Ù„ÙŠ Ø­Ù‚Ù‡\n",
        "                boxes.append(torch.Tensor([xmin, ymin, xmax, ymax]).to(device))\n",
        "                labels.append(voc_classes[label]) #  voc_classes ÙÙˆÙ‚ Ø¹Ø±ÙÙ†Ø§Ù‡ Ùˆ ÙŠØ·Ù„Ø¹ int\n",
        "\n",
        "            target['boxes'] = torch.stack(boxes)\n",
        "            target['labels'] = torch.Tensor(labels).type(torch.int64).to(device)\n",
        "\n",
        "        # Compute losses\n",
        "        loss_dict = model(images, targets) # stack of tensor from out loop and this tensor got the boxes ,, loss for boundery , loss of label tahts why there are dict\n",
        "        losses = sum(loss for loss in loss_dict.values())  # Sum all losses\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += losses.item() # we work with tesnsor and we used item to just get with one number of the Ø±Ù‚Ù…\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c407612",
      "metadata": {
        "id": "4c407612"
      },
      "source": [
        "### **ğŸ”¹ Validation Loop**\n",
        "#### **ğŸ”¹ How Do We Evaluate Object Detection Models?**\n",
        "To evaluate object detection models like **Faster R-CNN**, we need to measure **how well the predicted bounding boxes match the ground truth boxes**.\n",
        "\n",
        "![image.png](https://i.imgur.com/MDFxFMX.png)\n",
        "\n",
        "---\n",
        "\n",
        "#### **ğŸ“Œ Intersection over Union (IoU)**\n",
        "âœ… We consider a detection **correct** if the predicted box **overlaps significantly** with the ground truth box.  \n",
        "âœ… This is measured using **Intersection over Union (IoU)**, which calculates the **ratio of overlap** between the two boxes.\n",
        "\n",
        "$$\n",
        "IoU = \\frac{\\text{Area of Overlap}}{\\text{Area of Union}}\n",
        "$$\n",
        "\n",
        "ğŸš€ **Higher IoU = Better Detection!**  \n",
        "\n",
        "\n",
        "![image.png](https://i.imgur.com/yNNhjwr.png)\n",
        "\n",
        "---\n",
        "\n",
        "#### **ğŸ“Œ What is mAP@0.5:0.95?**\n",
        "mAP (**mean Average Precision**) is the most commonly used **metric for object detection**.\n",
        "\n",
        "ğŸ”¹ **mAP@0.5:0.95** means we compute the **average precision** at **different IoU thresholds** from **0.5 to 0.95**, increasing in steps of **0.05**.\n",
        "\n",
        "- **IoU â‰¥ 0.5** â†’ Loose match  \n",
        "- **IoU â‰¥ 0.75** â†’ Stricter match  \n",
        "- **IoU â‰¥ 0.95** â†’ Extremely strict match  \n",
        "\n",
        "**mAP@0.5:0.95** takes the **average of all these values**, giving us a single number that represents how well the model performs **across different difficulty levels**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cd2ac986ab9db3c",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5cd2ac986ab9db3c"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output ## IoU :: to see the number of ground and prediction how much they are overlapping to each other\n",
        "!pip install torchmetrics\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81209ee3",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "81209ee3"
      },
      "outputs": [],
      "source": [
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "\n",
        "# Initialize metric\n",
        "metric = MeanAveragePrecision(iou_thresholds=[0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]) # ÙŠØ§Ø®Ø° ÙƒÙ„ ØªØ±Ø§Ø´ Ù‡ÙˆÙ„Ø¯ Ùˆ ÙŠØ¬Ø±Ø¨ Ø¹Ù„ÙŠÙ‡ Ù„Ù„ Ø¨Ø±ÙŠØ¯ÙƒØª Ø¨Ø¹Ø¯ÙŠÙ† ÙŠØ¬Ù…Ø¹ Ø§Ù„ÙˆØ§Ø­Ø¯ Ù„Ù„ØµØ­\n",
        "\n",
        "def validate(model, dataloader, device):\n",
        "    \"\"\"Evaluates the model using mAP@0.5:0.95.\"\"\"\n",
        "    model.eval()\n",
        "    metric.reset()  # Ø¯Ø§Ø®Ù„ ÙƒÙ„ ephoche Ù„Ø§Ø²Ù… Ù†Ø³ÙˆÙŠ Ø±ÙŠØ³ÙŠØª ÙÙŠ Ø¯ÙŠØªÙƒØª\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in tqdm(dataloader):\n",
        "            images = [img.to(device) for img in images]\n",
        "            preds = model(images)\n",
        "\n",
        "            # Convert predictions to correct format Ø§Ù„Ø§Ù†Ù‡Ù… Ù…ØªÙˆÙ‚Ø¹ÙŠÙ† ÙŠÙƒÙˆÙ†Ùˆ ÙÙŠ specfic format\n",
        "            processed_preds = []\n",
        "            for pred in preds:\n",
        "                processed_preds.append({\n",
        "                    \"boxes\": pred[\"boxes\"].cpu(),\n",
        "                    \"scores\": pred[\"scores\"].cpu(),\n",
        "                    \"labels\": pred[\"labels\"].cpu()\n",
        "                })\n",
        "\n",
        "            # Convert ground truth targets\n",
        "            processed_targets = []\n",
        "            for target in targets:\n",
        "                gt_boxes = []\n",
        "                gt_labels = []\n",
        "                for obj in target['annotation']['object']:\n",
        "                    label = obj['name']\n",
        "                    box = obj['bndbox']\n",
        "                    xmin, ymin, xmax, ymax = [int(box[k]) for k in ['xmin', 'ymin', 'xmax', 'ymax']]\n",
        "                    gt_boxes.append([xmin, ymin, xmax, ymax])\n",
        "                    gt_labels.append(voc_classes[label])\n",
        "\n",
        "                processed_targets.append({\n",
        "                    \"boxes\": torch.tensor(gt_boxes).cpu(),  # need cpu due to mAP expected ,, for less compution\n",
        "                    \"labels\": torch.tensor(gt_labels).cpu()\n",
        "                })\n",
        "\n",
        "            # Update metric\n",
        "            metric.update(processed_preds, processed_targets)\n",
        "\n",
        "    return metric.compute()  # Compute final mAP scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc0abbe7",
      "metadata": {
        "id": "bc0abbe7"
      },
      "source": [
        "## **4ï¸âƒ£ Running Training & Validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35c395d8",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "35c395d8",
        "outputId": "27c413bf-c019-4cac-cdf7-925ff4403071"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2501/2501 [49:43<00:00,  1.19s/it]\n",
            " 10%|â–‰         | 482/4952 [10:18<1:34:45,  1.27s/it]"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
        "num_epochs = 10  # Set number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, device) # Ø³ÙˆÙŠÙ†Ø§Ù‡ Ø¯Ø§ÙŠØ±ÙƒØªÙ„ÙŠ Ø§Ù„Ø§Ù†Ù‡ Ù…ÙˆØ¯Ù„ ÙŠØ±Ø¬Ø¹Ù‡\n",
        "    mAP_results = validate(model, test_loader, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}\") # Ù†ØªÙŠØ¬Ù‡ Ø¹Ø§Ù„ÙŠÙ‡ ÙŠØ¹Ù†ÙŠ Ø§ÙØ¶Ù„ ÙŠØ¹Ù†ÙŠ Ù…ÙˆØ¯Ù„ local for both localization and globilazation\n",
        "    print(f\"mAP@0.5:0.95 for Test: {mAP_results['map']:.4f}\") # Ù‚Ø±ÙŠØ¨ Ù„Ù„ Ù‚Ø±Ø§ÙˆÙ†Ø¯ Ùˆ Ø¨Ø±ÙŠØ¯ÙƒØª"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0a7d245",
      "metadata": {
        "id": "d0a7d245"
      },
      "source": [
        "## **5ï¸âƒ£ Visualizing Predictions vs. Ground Truth**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "773032bb",
      "metadata": {
        "id": "773032bb"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torchvision.transforms.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Helper function to overlay Ground Truth & Predicted Boxes\n",
        "def visualize_gt_pred(img, gt_boxes, gt_annotations, pred_boxes, pred_annotations, title=\"\"):\n",
        "    fig, ax = plt.subplots(1, figsize=(8, 8))\n",
        "    ax.imshow(img)\n",
        "\n",
        "    # Plot Ground Truth in RED\n",
        "    for bbox, annotation in zip(gt_boxes, gt_annotations):\n",
        "        x_min, y_min, x_max, y_max = bbox\n",
        "        width = x_max - x_min\n",
        "        height = y_max - y_min\n",
        "\n",
        "        rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        plt.text(x_min, y_min - 5, annotation, color='r', fontsize=8, bbox=dict(facecolor='white', alpha=0.7))\n",
        "\n",
        "    # Plot Predictions in GREEN\n",
        "    for bbox, annotation in zip(pred_boxes, pred_annotations):\n",
        "        x_min, y_min, x_max, y_max = bbox\n",
        "        width = x_max - x_min\n",
        "        height = y_max - y_min\n",
        "\n",
        "        rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='g', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        plt.text(x_min, y_min - 5, annotation, color='g', fontsize=8, bbox=dict(facecolor='white', alpha=0.7))\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e1447c4",
      "metadata": {
        "id": "7e1447c4"
      },
      "outputs": [],
      "source": [
        "# Select 3 random test images Ø±Ø§Ù†Ø¯ÙˆÙ…\n",
        "test_indices = [445557,555333, 222]\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Create figure with 5Ã—2 subplots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(30, 60))\n",
        "axes = axes.ravel()  # Flatten axes for easy iteration Ù„Ù„Ù…ÙƒØ§Ù† Ùˆ Ø§Ù„Ù…ÙˆØ«Ø¹ Ù‡Ùˆ ÙŠÙØ¶Ù„ ÙŠÙƒÙˆÙ† ØµÙ ÙˆØ§Ø­Ø¯ Ù„Ùˆ ÙƒØ§Ù† Ù…Ø®ØªÙ„Ù Ù ÙÙ‡Ùˆ Ù…Ù‡Ù…\n",
        "\n",
        "for i, idx in enumerate(test_indices):  # iterate through images 3 to get it into test image and target from our data index\n",
        "    test_img, test_target = test_dataset[idx] # Ù„ÙˆØ¨ Ø¨Ø§Ø®Ø° Ù‚ÙŠÙ…Ù‡ Ø§Ø®ØªØ¨Ø§Ø± Ùˆ ØªØ±Ø¬Ø¹ Ù„Ù†Ø§ i ----\n",
        "\n",
        "    # Extract Ground Truth Boxes & Labels\n",
        "    gt_boxes = []  # ground truth  Ù‚ÙŠÙ…Ù‡ ØµØ­ÙŠØ­Ù‡ ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "    gt_annotations = []  # labels\n",
        "    # one image  iterate to more than bounding box\n",
        "    for obj in test_target['annotation']['object']:\n",
        "        box = obj['bndbox']\n",
        "        xmin, ymin, xmax, ymax = [int(box[k]) for k in ['xmin', 'ymin', 'xmax', 'ymax']]\n",
        "        gt_boxes.append([xmin, ymin, xmax, ymax])\n",
        "        gt_annotations.append(obj['name'])\n",
        "\n",
        "    # Run Model on Test Image\n",
        "    with torch.no_grad():\n",
        "        pred = model([test_img.to(device)])\n",
        "\n",
        "    pred = pred[0]  # getting one image\n",
        "\n",
        "    # Extract Predictions\n",
        "    pred_boxes = pred['boxes'].cpu()\n",
        "    pred_annotations = pred['labels'].cpu()\n",
        "    pred_scores = pred['scores'].cpu() # Ù…ÙˆØ¬ÙˆØ¯Ù‡ Ø¨ÙŠÙ†Ù‡Ù… Ø¨ÙˆÙƒØ³ Ùˆ Ù„Ø§Ø¨Ù„\n",
        "\n",
        "    # Apply Confidence Threshold (Only keep predictions with score â‰¥ 0.8) Ø§Ø¹Ø±Ù Ø³ÙƒÙˆØ± Ù…Ù† Ù…Ø®Ø²Ù†\n",
        "    valid_mask = pred_scores >= 0.8 # ture or false ,, since it is expression\n",
        "    pred_annotations = pred_annotations[valid_mask]  # ÙŠØ§Ø®Ø° Ø§Ù„Ù‚ÙŠÙ…Ù‡ Ø¨Ø³ Ø§Ù„ØµØ­ÙŠØ­Ù‡\n",
        "    pred_boxes = pred_boxes[valid_mask]\n",
        "\n",
        "    # Convert Predicted Labels from Numeric to Class Names\n",
        "    pred_annotations = [reverse_voc_classes[val.item()] for val in pred_annotations]   # Ù‡Ùˆ Ø±Ù‚Ù… Ø¨Ø³ Ø¹Ù„Ø´Ø§Ù† Ø§Ø·Ù„Ø¹Ù‡ ÙÙŠ Ø´Ø§Ø±Øª Ø§Ø±Ø¬Ù‡ Ø³ØªØ±ÙŠÙ†Ù‚\n",
        "\n",
        "    # Overlay GT & Predictions on Image ØµÙˆØ± Ù‡Ùˆ ØªÙŠÙ†Ø³ÙˆØ±Ø± Ù Ù†Ø±Ø¬Ø¹Ù‡ Ù„Ù„ØµÙˆØ±Ù‡ Ù…Ù† Ø®Ù„Ø§Ù„ Ù‡Ø°Ø§\n",
        "    img = F.to_pil_image(test_img)\n",
        "    ax = axes[i]\n",
        "    ax.imshow(img)\n",
        "\n",
        "    # Plot Ground Truth in RED  Ù†Ø­Ø· Ù‚Ø±Ø§ÙˆÙ†Ø¯ Ø¨ÙˆÙƒØ³ Ùˆ Ù„Ø§Ø¨Ù„\n",
        "    for bbox, annotation in zip(gt_boxes, gt_annotations):  # zip to iterat OVER MULTIPLE VALUES\n",
        "        x_min, y_min, x_max, y_max = bbox\n",
        "        width = x_max - x_min\n",
        "        height = y_max - y_min\n",
        "\n",
        "        rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')  # object is Ø±ÙƒØªØ§Ù†Ù‚Ù„ there are library to draw them\n",
        "        #  linewidth Ø³Ù…Ø§ÙƒØ©  facecolor fill color\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x_min, y_min - 5, annotation, color='r', fontsize=24, bbox=dict(facecolor='white', alpha=0.7))  # x_min, y_min - 5 Ø·Ø¨Ø§Ø¹Ù‡ Ù„Ø§Ø¨Ù„ Ø¹Ù„ Ø§Ø¨Ø± Ø¨Ø§Ø±Øª Ù„Ø§ÙØª\n",
        "\n",
        "    # Plot Predictions in GREEN\n",
        "    for bbox, annotation in zip(pred_boxes, pred_annotations):\n",
        "        x_min, y_min, x_max, y_max = bbox\n",
        "        width = x_max - x_min\n",
        "        height = y_max - y_min\n",
        "\n",
        "        rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='g', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x_min, y_min - 5, annotation, color='g', fontsize=24, bbox=dict(facecolor='white', alpha=0.7))\n",
        "\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f\"Image {idx}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd0f36d4",
      "metadata": {
        "id": "bd0f36d4"
      },
      "source": [
        "### Contributed by: Mohamed Eltayeb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4d695b2",
      "metadata": {
        "id": "d4d695b2"
      },
      "source": [
        "![image.png](https://i.imgur.com/a3uAqnb.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2a12c26",
      "metadata": {
        "id": "a2a12c26"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30840,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 2046.963277,
      "end_time": "2025-02-02T15:48:34.514608",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-02-02T15:14:27.551331",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}